
import streamlit as st
import numpy as np
import librosa
from streamlit_webrtc import webrtc_streamer
import av
import joblib
model = joblib.load("alarm_model.pkl")

# ğŸ¨ Page Configuration
st.set_page_config(page_title="ğŸ”” Alarm Classifier", layout="centered", page_icon="ğŸ§")

# ğŸŒ— Toggle Dark Mode
theme = st.toggle("ğŸŒ— Toggle Dark Mode", value=False)
bg_color = "#1E1E1E" if theme else "#FFFFFF"
text_color = "#FFFFFF" if theme else "#000000"

st.markdown(
    f"""
    <style>
    .dark-container {{
        background-color: {bg_color};
        color: {text_color};
        padding: 30px;
        border-radius: 10px;
    }}
    .stButton > button {{
        background-color: #0072B2;
        color: white;
    }}
    </style>
    <div class="dark-container">
    """,
    unsafe_allow_html=True
)

# ğŸ”Š Header
st.markdown("<h1 style='text-align:center;'>ğŸ”Š Real Alarm Classifier</h1>", unsafe_allow_html=True)

# ğŸ§  How it works
with st.expander("ğŸ§  How does this work?"):
    st.markdown("""
    - Upload a `.wav` file or use your microphone.
    - Detects types of alarms or background sounds.
    - Trained with real-world audio samples.
    """)

# ğŸ­ Sound Classes (must match labels used during training)
ALL_CLASSES = {
    "fire_alarm": "ğŸ”¥",
    "buzzer": "ğŸ›ï¸",
    "smoke_detector": "ğŸš¨",
    "timer_alarm": "â°"
    # Add more classes if you train on additional categories
}
CLASSES = list(ALL_CLASSES.keys())

# ğŸ” Load the trained model
MODEL_PATH = "alarm_model.pkl"
try:
    model = joblib.load(MODEL_PATH)
    # If the model has a `classes_` attribute, ensure it's defined
    if not hasattr(model, "classes_"):
        st.error("Loaded model does not have `classes_`. Did you train & save correctly?")
        model = None
except FileNotFoundError:
    st.error(f"Model file not found at {MODEL_PATH}. Please run `train_model.py` first.")
    model = None

# ğŸ§ª Feature Extraction (must match training)
def extract_features(y, sr):
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    mfccs_mean = np.mean(mfccs, axis=1)
    chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=12)
    chroma_mean = np.mean(chroma, axis=1)
    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)
    contrast_mean = np.mean(contrast, axis=1)
    zcr = librosa.feature.zero_crossing_rate(y)
    zcr_mean = np.mean(zcr)
    return np.hstack([mfccs_mean, chroma_mean, contrast_mean, zcr_mean])

# ğŸ“‚ Tabs
tab1, tab2 = st.tabs(["ğŸ“‚ Upload File", "ğŸ¤ Microphone"])

# ğŸ“‚ Upload Tab
with tab1:
    uploaded_file = st.file_uploader("Upload a WAV file", type=["wav"])
    if uploaded_file and model:
        with st.spinner("Analyzing audio..."):
            y, sr = librosa.load(uploaded_file, sr=None, duration=5.0)
            features = extract_features(y, sr).reshape(1, -1)
            if features.shape[1] == model.n_features_in_:
                pred = model.predict(features)[0]
                probs = model.predict_proba(features)[0]
                confidence = np.max(probs)
                st.success(f"{ALL_CLASSES[pred]} *{pred.replace('_', ' ').title()}* detected!")
                st.progress(confidence)
                st.toast(f"âœ… {int(confidence*100)}% confidence", icon="ğŸ“Š")
            else:
                st.error("âš ï¸ Feature size mismatch.")

# ğŸ¤ Live Mic Tab
with tab2:
    st.markdown("### ğŸ™ï¸ Real-time Microphone Input")
    st.caption("Allow mic permissions in your browser.")

    if "live_prediction" not in st.session_state:
        st.session_state["live_prediction"] = "Waiting..."

    def audio_callback(frame: av.AudioFrame):
        audio = frame.to_ndarray(format="flt32")
        if audio.ndim > 1:
            audio = audio.mean(axis=0)
        sr = frame.sample_rate
        features = extract_features(audio, sr).reshape(1, -1)
        if features.shape[1] == model.n_features_in_:
            pred = model.predict(features)[0]
            st.session_state["live_prediction"] = f"{ALL_CLASSES.get(pred)} {pred.replace('_', ' ').title()}"
        else:
            st.session_state["live_prediction"] = "âš ï¸ Feature error"
        return frame

    if model:
        webrtc_streamer(
            key="live-audio",
            audio_frame_callback=audio_callback,
            media_stream_constraints={"audio": True, "video": False},
            async_processing=True,
        )
        st.info(f"ğŸ”” Real-time: *{st.session_state['live_prediction']}*")

# ğŸ“˜ Legend
with st.expander("ğŸ“˜ Sound Labels"):
    for name, emoji in ALL_CLASSES.items():
        st.markdown(f"- {emoji} **{name.replace('_', ' ').title()}**")

# ğŸ“ Footer
st.markdown("---")
st.caption("ğŸ”§ Built with Streamlit Â· Uses real ML model for alarm classification")

st.markdown("</div>", unsafe_allow_html=True)
